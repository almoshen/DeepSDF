{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3\n","# Copyright 2004-present Facebook. All Rights Reserved."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import argparse\n","import json\n","import logging\n","import os\n","import random\n","import time\n","import torch"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import deep_sdf\n","import deep_sdf.workspace as ws\n","import deep_sdf.data as deep_data\n","import deep_sdf.mesh as deep_mesh"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def reconstruct(\n","    decoder,\n","    num_iterations,\n","    latent_size,\n","    test_sdf,\n","    stat,\n","    clamp_dist,\n","    num_samples=30000,\n","    lr=5e-4,\n","    l2reg=False,\n","):\n","    def adjust_learning_rate(\n","        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n","    ):\n","        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n","        for param_group in optimizer.param_groups:\n","            param_group[\"lr\"] = lr\n","    decreased_by = 10\n","    adjust_lr_every = int(num_iterations / 2)\n","    if type(stat) == type(0.1):\n","        latent = torch.ones(1, latent_size).normal_(mean=0, std=stat).cuda()\n","    else:\n","        latent = torch.normal(stat[0].detach(), stat[1].detach()).cuda()\n","    latent.requires_grad = True\n","    optimizer = torch.optim.Adam([latent], lr=lr)\n","    loss_num = 0\n","    loss_l1 = torch.nn.L1Loss()\n","    for e in range(num_iterations):\n","        decoder.eval()\n","        sdf_data = deep_data.unpack_sdf_samples_from_ram(\n","            test_sdf, num_samples\n","        ).cuda()\n","        # sdf_dataset = deep_sdf.data.MSDFSamples(\n","        # data_source, train_split, num_samp_per_scene\n","        # ).cuda()\n","        xyz = sdf_data[:, 0:3]\n","        sdf_gt = sdf_data[:, 3].unsqueeze(1)\n","        sdf_gt = torch.clamp(sdf_gt, -clamp_dist, clamp_dist)\n","        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n","        optimizer.zero_grad()\n","        latent_inputs = latent.expand(num_samples, -1)\n","        inputs = torch.cat([latent_inputs, xyz], 1).cuda()\n","        pred_sdf = decoder(inputs)\n","\n","        # TODO: why is this needed?\n","        if e == 0:\n","            pred_sdf = decoder(inputs)\n","        pred_sdf = torch.clamp(pred_sdf, -clamp_dist, clamp_dist)\n","        loss = loss_l1(pred_sdf, sdf_gt)\n","        if l2reg:\n","            loss += 1e-4 * torch.mean(latent.pow(2))\n","        loss.backward()\n","        optimizer.step()\n","        if e % 50 == 0:\n","            logging.debug(loss.cpu().data.numpy())\n","            logging.debug(e)\n","            logging.debug(latent.norm())\n","        loss_num = loss.cpu().data.numpy()\n","    return loss_num, latent"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_filenames(data_source, split):\n","    folders = []\n","    print(data_source)\n","    for dataset in split:\n","        for class_name in split[dataset]:\n","            for instance_name in split[dataset][class_name]:\n","                instance_filename = os.path.join(\n","                    dataset, class_name, instance_name\n","                )\n","                folder_path = os.path.join(data_source, ws.sdf_samples_subdir, instance_filename)\n","                if os.path.isdir(folder_path):\n","                    if not os.listdir(folder_path):\n","                        print(folder_path + ' is empty')\n","                    else:\n","                        folders += [instance_filename]\n","                else:\n","                    print(folder_path + ' does not exist')\n","                \n","    return folders"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reconstruct time: 12.591426134109497\n","current_error avg: 0.0013538023922592402\n","sampling takes: 3.422968\n","reconstruct time: 11.64418649673462\n","current_error avg: 0.0010975312907248735\n","sampling takes: 3.392440\n","reconstruct time: 11.812104225158691\n","current_error avg: 0.0010157341215138633\n","sampling takes: 3.392238\n","reconstruct time: 11.880801439285278\n","current_error avg: 0.0009861653379630297\n","sampling takes: 3.405319\n","reconstruct time: 11.802763223648071\n","current_error avg: 0.0009567340603098273\n","sampling takes: 3.417471\n","reconstruct time: 12.007084608078003\n","current_error avg: 0.000979097463035335\n","sampling takes: 3.480113\n","reconstruct time: 12.232516288757324\n","current_error avg: 0.0009652170868191336\n","sampling takes: 3.529151\n"]}],"source":["iterations = 2000\n","experiment_directory = 'examples/sofas'\n","specs = ws.load_experiment_specifications(experiment_directory)\n","checkpoint = 'latest'\n","split_filename = specs[\"TestSplit\"]\n","data_source = specs[\"DataSource\"]\n","skip = False\n","\n","def empirical_stat(latent_vecs, indices):\n","        lat_mat = torch.zeros(0).cuda()\n","        for ind in indices:\n","            lat_mat = torch.cat([lat_mat, latent_vecs[ind]], 0)\n","        mean = torch.mean(lat_mat, 0)\n","        var = torch.var(lat_mat, 0)\n","        return mean, var\n","\n","# if not os.path.isfile(specs_filename):\n","#     raise Exception(\n","#         'The experiment directory does not include specifications file \"specs.json\"'\n","#     )\n","\n","# specs = json.load(open(specs_filename))\n","\n","arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n","\n","latent_size = specs[\"CodeLength\"]\n","\n","decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n","\n","decoder = torch.nn.DataParallel(decoder)\n","\n","saved_model_state = torch.load(\n","    os.path.join(\n","        experiment_directory, ws.model_params_subdir, checkpoint + \".pth\"\n","        )\n","    )\n","saved_model_epoch = saved_model_state[\"epoch\"]\n","\n","decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n","\n","decoder = decoder.module.cuda()\n","\n","with open(split_filename, \"r\") as f:\n","    split = json.load(f)\n","\n","    foldernames = deep_sdf.data.get_filenames(data_source, split)\n","\n","    random.shuffle(foldernames)\n","\n","    logging.debug(decoder)\n","\n","    err_sum = 0.0\n","    repeat = 1\n","    save_latvec_only = False\n","    rerun = 0\n","\n","    reconstruction_dir = os.path.join(\n","        experiment_directory, ws.reconstructions_subdir, str(saved_model_epoch)\n","    )\n","\n","    if not os.path.isdir(reconstruction_dir):\n","        os.makedirs(reconstruction_dir)\n","\n","    reconstruction_meshes_dir = os.path.join(\n","        reconstruction_dir, ws.reconstruction_meshes_subdir\n","    )\n","    if not os.path.isdir(reconstruction_meshes_dir):\n","        os.makedirs(reconstruction_meshes_dir)\n","\n","    reconstruction_codes_dir = os.path.join(\n","        reconstruction_dir, ws.reconstruction_codes_subdir\n","    )\n","    if not os.path.isdir(reconstruction_codes_dir):\n","        os.makedirs(reconstruction_codes_dir)\n","    \n","    for ii, folder in enumerate(foldernames):\n","\n","\n","        full_filename = os.path.join(data_source, ws.sdf_samples_subdir, folder)\n","\n","        if not os.path.isdir(full_filename):\n","            continue\n","\n","        if not os.listdir(full_filename):\n","            continue\n","\n","        data_sdf = deep_data.read_sdfs_into_ram(full_filename)\n","\n","        for k in range(repeat):\n","\n","            if rerun > 1:\n","                mesh_filename = os.path.join(\n","                    reconstruction_meshes_dir, folder + \"-\" + str(k + rerun)\n","                )\n","                latent_filename = os.path.join(\n","                    reconstruction_codes_dir, folder + \"-\" + str(k + rerun) + \".pth\"\n","                )\n","            else:\n","                mesh_filename = os.path.join(reconstruction_meshes_dir, folder)\n","                latent_filename = os.path.join(\n","                    reconstruction_codes_dir, folder + \".pth\"\n","                )\n","\n","            if (\n","                skip\n","                and os.path.isfile(mesh_filename + \".ply\")\n","                and os.path.isfile(latent_filename)\n","            ):\n","                continue\n","\n","\n","            data_sdf[0] = data_sdf[0][torch.randperm(data_sdf[0].shape[0])]\n","            data_sdf[1] = data_sdf[1][torch.randperm(data_sdf[1].shape[0])]\n","\n","            start = time.time()\n","            err, latent = reconstruct(\n","                decoder,\n","                int(iterations),\n","                latent_size,\n","                data_sdf,\n","                0.01,  # [emp_mean,emp_var],\n","                0.1,\n","                num_samples=8000,\n","                lr=5e-3,\n","                l2reg=True,\n","            )\n","            logging.debug(\"reconstruct time: {}\".format(time.time() - start))\n","            print(\"reconstruct time: {}\".format(time.time() - start))\n","            err_sum += err\n","            logging.debug(\"current_error avg: {}\".format((err_sum / (ii + 1))))\n","            print(\"current_error avg: {}\".format((err_sum / (ii + 1))))\n","            logging.debug(ii)\n","\n","            logging.debug(\"latent: {}\".format(latent.detach().cpu().numpy()))\n","\n","            decoder.eval()\n","\n","            if not os.path.exists(os.path.dirname(mesh_filename)):\n","                os.makedirs(os.path.dirname(mesh_filename))\n","\n","            if not save_latvec_only:\n","                start = time.time()\n","                with torch.no_grad():\n","                    deep_mesh.create_mesh(\n","                        decoder, latent, mesh_filename, N=256, max_batch=int(2 ** 18)\n","                    )\n","                logging.debug(\"total time: {}\".format(time.time() - start))\n","                print(\"total time: {}\".format(time.time() - start))\n","\n","            if not os.path.exists(os.path.dirname(latent_filename)):\n","                os.makedirs(os.path.dirname(latent_filename))\n","\n","            torch.save(latent.unsqueeze(0), latent_filename)"]}],"metadata":{"kernelspec":{"display_name":"deepSDF","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"6b8269542496d3a0fb6a6c002ebf06e81bd408d243997e872c59b60e4b1e04cf"}}},"nbformat":4,"nbformat_minor":2}
