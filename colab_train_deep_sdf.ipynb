{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3\n","# Copyright 2004-present Facebook. All Rights Reserved."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3>Execuation</h3>\n","Run main_function -> load json which contains experiment specifications\n","\n","specs: specs.json<br>\n","datasource: Data folder <br>\n","train_split_file: examples/splits/sv2_chairs_train.json<br>\n","arch: import Decoder from networks.deep_sdf_decoder.py<br>\n","checkpoints:[100, 500, 1000, 2000]<br>\n","lr_schedules: list of 2 objects containing learning rate<br>\n","grad_clip: gradient clipping none<br>\n","\n","save_latest(): 1.save_model():save parameters (latest.pth) to examples/sofas/ModelParameters 2.save_optimizer: save optimizer's state_dict to OptimizerParameters 3.save_latent_vectors: to LatentCodes<br>\n","signal_handler: ctrl+c stop early<br>\n","num_samp_per_scene: 16384 SDF samples for each shape in a batch<br>\n","scene_per_batch: 64 shapes<br>\n","clamp_dist: 0.1 control the distance from the surface over which we expect to maintain a metric SDF<br>\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd /content/drive/Shareddrives/Deep Elastics/Shen Projects/Github/DeepSDF"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# !pip install plyfile"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# !pip install trimesh"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3>Import packages<h3>"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torch\n","import torch.utils.data as data_utils\n","import signal\n","import sys\n","import os\n","import logging\n","import math\n","import json\n","import time"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["use workspace to load json file"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import deep_sdf\n","import deep_sdf.workspace as ws"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class LearningRateSchedule:\n","    def get_learning_rate(self, epoch):\n","        pass"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class ConstantLearningRateSchedule(LearningRateSchedule):\n","    def __init__(self, value):\n","        self.value = value\n","    def get_learning_rate(self, epoch):\n","        return self.value"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class StepLearningRateSchedule(LearningRateSchedule):\n","    def __init__(self, initial, interval, factor):\n","        self.initial = initial\n","        self.interval = interval\n","        self.factor = factor\n","    def get_learning_rate(self, epoch):\n","        return self.initial * (self.factor ** (epoch // self.interval))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class WarmupLearningRateSchedule(LearningRateSchedule):\n","    def __init__(self, initial, warmed_up, length):\n","        self.initial = initial\n","        self.warmed_up = warmed_up\n","        self.length = length\n","    def get_learning_rate(self, epoch):\n","        if epoch > self.length:\n","            return self.warmed_up\n","        return self.initial + (self.warmed_up - self.initial) * epoch / self.length"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def get_learning_rate_schedules(specs):\n","    schedule_specs = specs[\"LearningRateSchedule\"]\n","    schedules = []\n","    for schedule_specs in schedule_specs:\n","        if schedule_specs[\"Type\"] == \"Step\":\n","            schedules.append(\n","                StepLearningRateSchedule(\n","                    schedule_specs[\"Initial\"],\n","                    schedule_specs[\"Interval\"],\n","                    schedule_specs[\"Factor\"],\n","                )\n","            )\n","        elif schedule_specs[\"Type\"] == \"Warmup\":\n","            schedules.append(\n","                WarmupLearningRateSchedule(\n","                    schedule_specs[\"Initial\"],\n","                    schedule_specs[\"Final\"],\n","                    schedule_specs[\"Length\"],\n","                )\n","            )\n","        elif schedule_specs[\"Type\"] == \"Constant\":\n","            schedules.append(ConstantLearningRateSchedule(schedule_specs[\"Value\"]))\n","        else:\n","            raise Exception(\n","                'no known learning rate schedule of type \"{}\"'.format(\n","                    schedule_specs[\"Type\"]\n","                )\n","            )\n","    return schedules"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def save_model(experiment_directory, filename, decoder, epoch):\n","    model_params_dir = ws.get_model_params_dir(experiment_directory, True)\n","    torch.save(\n","        {\"epoch\": epoch, \"model_state_dict\": decoder.state_dict()},\n","        os.path.join(model_params_dir, filename),\n","    )"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def save_optimizer(experiment_directory, filename, optimizer, epoch):\n","    optimizer_params_dir = ws.get_optimizer_params_dir(experiment_directory, True)\n","    torch.save(\n","        {\"epoch\": epoch, \"optimizer_state_dict\": optimizer.state_dict()},\n","        os.path.join(optimizer_params_dir, filename),\n","    )"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def load_optimizer(experiment_directory, filename, optimizer):\n","    full_filename = os.path.join(\n","        ws.get_optimizer_params_dir(experiment_directory), filename\n","    )\n","    if not os.path.isfile(full_filename):\n","        raise Exception(\n","            'optimizer state dict \"{}\" does not exist'.format(full_filename)\n","        )\n","    data = torch.load(full_filename)\n","    optimizer.load_state_dict(data[\"optimizer_state_dict\"])\n","    return data[\"epoch\"]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def save_latent_vectors(experiment_directory, filename, latent_vec, epoch):\n","    latent_codes_dir = ws.get_latent_codes_dir(experiment_directory, True)\n","    all_latents = latent_vec.state_dict()\n","    torch.save(\n","        {\"epoch\": epoch, \"latent_codes\": all_latents},\n","        os.path.join(latent_codes_dir, filename),\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["TODO: duplicated in workspace"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def load_latent_vectors(experiment_directory, filename, lat_vecs):\n","    full_filename = os.path.join(\n","        ws.get_latent_codes_dir(experiment_directory), filename\n","    )\n","    if not os.path.isfile(full_filename):\n","        raise Exception('latent state file \"{}\" does not exist'.format(full_filename))\n","    data = torch.load(full_filename)\n","    if isinstance(data[\"latent_codes\"], torch.Tensor):\n","\n","        # for backwards compatibility\n","        if not lat_vecs.num_embeddings == data[\"latent_codes\"].size()[0]:\n","            raise Exception(\n","                \"num latent codes mismatched: {} vs {}\".format(\n","                    lat_vecs.num_embeddings, data[\"latent_codes\"].size()[0]\n","                )\n","            )\n","        if not lat_vecs.embedding_dim == data[\"latent_codes\"].size()[2]:\n","            raise Exception(\"latent code dimensionality mismatch\")\n","        for i, lat_vec in enumerate(data[\"latent_codes\"]):\n","            lat_vecs.weight.data[i, :] = lat_vec\n","    else:\n","        lat_vecs.load_state_dict(data[\"latent_codes\"])\n","    return data[\"epoch\"]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def save_logs(\n","    experiment_directory,\n","    loss_log,\n","    lr_log,\n","    timing_log,\n","    lat_mag_log,\n","    param_mag_log,\n","    epoch,\n","):\n","    torch.save(\n","        {\n","            \"epoch\": epoch,\n","            \"loss\": loss_log,\n","            \"learning_rate\": lr_log,\n","            \"timing\": timing_log,\n","            \"latent_magnitude\": lat_mag_log,\n","            \"param_magnitude\": param_mag_log,\n","        },\n","        os.path.join(experiment_directory, ws.logs_filename),\n","    )"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def load_logs(experiment_directory):\n","    full_filename = os.path.join(experiment_directory, ws.logs_filename)\n","    if not os.path.isfile(full_filename):\n","        raise Exception('log file \"{}\" does not exist'.format(full_filename))\n","    data = torch.load(full_filename)\n","    return (\n","        data[\"loss\"],\n","        data[\"learning_rate\"],\n","        data[\"timing\"],\n","        data[\"latent_magnitude\"],\n","        data[\"param_magnitude\"],\n","        data[\"epoch\"],\n","    )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def clip_logs(loss_log, lr_log, timing_log, lat_mag_log, param_mag_log, epoch):\n","    iters_per_epoch = len(loss_log) // len(lr_log)\n","    loss_log = loss_log[: (iters_per_epoch * epoch)]\n","    lr_log = lr_log[:epoch]\n","    timing_log = timing_log[:epoch]\n","    lat_mag_log = lat_mag_log[:epoch]\n","    for n in param_mag_log:\n","        param_mag_log[n] = param_mag_log[n][:epoch]\n","    return (loss_log, lr_log, timing_log, lat_mag_log, param_mag_log)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def get_spec_with_default(specs, key, default):\n","    try:\n","        return specs[key]\n","    except KeyError:\n","        return default"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def get_mean_latent_vector_magnitude(latent_vectors):\n","    return torch.mean(torch.norm(latent_vectors.weight.data.detach(), dim=1))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def append_parameter_magnitudes(param_mag_log, model):\n","    for name, param in model.named_parameters():\n","        if len(name) > 7 and name[:7] == \"module.\":\n","            name = name[7:]\n","        if name not in param_mag_log.keys():\n","            param_mag_log[name] = []\n","        param_mag_log[name].append(param.data.norm().item())"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def main_function(experiment_directory, continue_from=None, batch_split=1):\n","    logging.debug(\"running \" + experiment_directory)\n","    specs = ws.load_experiment_specifications(experiment_directory)\n","    data_source = specs[\"DataSource\"]\n","    train_split_file = specs[\"TrainSplit\"]\n","    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n","    logging.debug(specs[\"NetworkSpecs\"])\n","    latent_size = specs[\"CodeLength\"]\n","    checkpoints = list(\n","        range(\n","            specs[\"SnapshotFrequency\"],\n","            specs[\"NumEpochs\"] + 1,\n","            specs[\"SnapshotFrequency\"],\n","        )\n","    )\n","    for checkpoint in specs[\"AdditionalSnapshots\"]:\n","        checkpoints.append(checkpoint)\n","    checkpoints.sort()\n","    lr_schedules = get_learning_rate_schedules(specs)\n","    grad_clip = get_spec_with_default(specs, \"GradientClipNorm\", None)\n","    if grad_clip is not None:\n","        logging.debug(\"clipping gradients to max norm {}\".format(grad_clip))\n","    def save_latest(epoch):\n","        save_model(experiment_directory, \"latest.pth\", decoder, epoch)\n","        save_optimizer(experiment_directory, \"latest.pth\", optimizer_all, epoch)\n","        save_latent_vectors(experiment_directory, \"latest.pth\", lat_vecs, epoch)\n","    def save_checkpoints(epoch):\n","        save_model(experiment_directory, str(epoch) + \".pth\", decoder, epoch)\n","        save_optimizer(experiment_directory, str(epoch) + \".pth\", optimizer_all, epoch)\n","        save_latent_vectors(experiment_directory, str(epoch) + \".pth\", lat_vecs, epoch)\n","    def signal_handler(sig, frame):\n","        logging.info(\"Stopping early...\")\n","        sys.exit(0)\n","    def adjust_learning_rate(lr_schedules, optimizer, epoch):\n","        for i, param_group in enumerate(optimizer.param_groups):\n","            param_group[\"lr\"] = lr_schedules[i].get_learning_rate(epoch)\n","    def empirical_stat(latent_vecs, indices):\n","        lat_mat = torch.zeros(0).cuda()\n","        for ind in indices:\n","            lat_mat = torch.cat([lat_mat, latent_vecs[ind]], 0)\n","        mean = torch.mean(lat_mat, 0)\n","        var = torch.var(lat_mat, 0)\n","        return mean, var\n","    signal.signal(signal.SIGINT, signal_handler)\n","    num_samp_per_scene = specs[\"SamplesPerScene\"]\n","    scene_per_batch = specs[\"ScenesPerBatch\"]\n","    clamp_dist = specs[\"ClampingDistance\"]\n","    minT = -clamp_dist\n","    maxT = clamp_dist\n","    enforce_minmax = True\n","    do_code_regularization = get_spec_with_default(specs, \"CodeRegularization\", True)\n","    code_reg_lambda = get_spec_with_default(specs, \"CodeRegularizationLambda\", 1e-4)\n","    code_bound = get_spec_with_default(specs, \"CodeBound\", None)\n","    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"]).cuda()\n","    logging.info(\"training with {} GPU(s)\".format(torch.cuda.device_count()))\n","    print(\"training with {} GPU(s)\".format(torch.cuda.device_count()))\n","\n","    # if torch.cuda.device_count() > 1:\n","    decoder = torch.nn.DataParallel(decoder)\n","    num_epochs = specs[\"NumEpochs\"]\n","    log_frequency = get_spec_with_default(specs, \"LogFrequency\", 10)\n","    with open(train_split_file, \"r\") as f:\n","        train_split = json.load(f)\n","    # sdf_dataset = deep_sdf.data.SDFSamples(\n","    #     data_source, train_split, num_samp_per_scene, load_ram=False\n","    # )\n","    sdf_dataset = deep_sdf.data.MSDFSamples(\n","        data_source, train_split, num_samp_per_scene\n","    )\n","    \n","    num_data_loader_threads = get_spec_with_default(specs, \"DataLoaderThreads\", 1)\n","    logging.debug(\"loading data with {} threads\".format(num_data_loader_threads))\n","    sdf_loader = data_utils.DataLoader(\n","        sdf_dataset,\n","        batch_size=scene_per_batch,\n","        shuffle=True,\n","        num_workers=num_data_loader_threads,\n","        drop_last=True,\n","    )\n","    logging.debug(\"torch num_threads: {}\".format(torch.get_num_threads()))\n","    num_scenes = len(sdf_dataset)\n","    logging.info(\"There are {} scenes\".format(num_scenes))\n","    logging.debug(decoder)\n","    lat_vecs = torch.nn.Embedding(num_scenes, latent_size, max_norm=code_bound)\n","    torch.nn.init.normal_(\n","        lat_vecs.weight.data,\n","        0.0,\n","        get_spec_with_default(specs, \"CodeInitStdDev\", 1.0) / math.sqrt(latent_size),\n","    )\n","    logging.debug(\n","        \"initialized with mean magnitude {}\".format(\n","            get_mean_latent_vector_magnitude(lat_vecs)\n","        )\n","    )\n","    loss_l1 = torch.nn.L1Loss(reduction=\"sum\")\n","    optimizer_all = torch.optim.Adam(\n","        [\n","            {\n","                \"params\": decoder.parameters(),\n","                \"lr\": lr_schedules[0].get_learning_rate(0),\n","            },\n","            {\n","                \"params\": lat_vecs.parameters(),\n","                \"lr\": lr_schedules[1].get_learning_rate(0),\n","            },\n","        ]\n","    )\n","    loss_log = []\n","    lr_log = []\n","    lat_mag_log = []\n","    timing_log = []\n","    param_mag_log = {}\n","    start_epoch = 1\n","    if continue_from is not None:\n","        logging.info('continuing from \"{}\"'.format(continue_from))\n","        print('continuing from \"{}\"'.format(continue_from))\n","        lat_epoch = load_latent_vectors(\n","            experiment_directory, continue_from + \".pth\", lat_vecs\n","        )\n","        model_epoch = ws.load_model_parameters(\n","            experiment_directory, continue_from, decoder\n","        )\n","        optimizer_epoch = load_optimizer(\n","            experiment_directory, continue_from + \".pth\", optimizer_all\n","        )\n","        loss_log, lr_log, timing_log, lat_mag_log, param_mag_log, log_epoch = load_logs(\n","            experiment_directory\n","        )\n","        if not log_epoch == model_epoch:\n","            loss_log, lr_log, timing_log, lat_mag_log, param_mag_log = clip_logs(\n","                loss_log, lr_log, timing_log, lat_mag_log, param_mag_log, model_epoch\n","            )\n","        if not (model_epoch == optimizer_epoch and model_epoch == lat_epoch):\n","            raise RuntimeError(\n","                \"epoch mismatch: {} vs {} vs {} vs {}\".format(\n","                    model_epoch, optimizer_epoch, lat_epoch, log_epoch\n","                )\n","            )\n","        start_epoch = model_epoch + 1\n","        logging.debug(\"loaded\")\n","    logging.info(\"starting from epoch {}\".format(start_epoch))\n","    logging.info(\n","        \"Number of decoder parameters: {}\".format(\n","            sum(p.data.nelement() for p in decoder.parameters())\n","        )\n","    )\n","    logging.info(\n","        \"Number of shape code parameters: {} (# codes {}, code dim {})\".format(\n","            lat_vecs.num_embeddings * lat_vecs.embedding_dim,\n","            lat_vecs.num_embeddings,\n","            lat_vecs.embedding_dim,\n","        )\n","    )\n","    for epoch in range(start_epoch, num_epochs + 1):\n","        start = time.time()\n","        logging.info(\"epoch {}...\".format(epoch))\n","        decoder.train()\n","        adjust_learning_rate(lr_schedules, optimizer_all, epoch)\n","        for sdf_data, indices in sdf_loader:\n","\n","            # Process the input data\n","            sdf_data = sdf_data.reshape(-1, 4)\n","            num_sdf_samples = sdf_data.shape[0]\n","            sdf_data.requires_grad = False\n","            xyz = sdf_data[:, 0:3]\n","            sdf_gt = sdf_data[:, 3].unsqueeze(1)\n","            if enforce_minmax:\n","                sdf_gt = torch.clamp(sdf_gt, minT, maxT)\n","            xyz = torch.chunk(xyz, batch_split)\n","            indices = torch.chunk(\n","                indices.unsqueeze(-1).repeat(1, num_samp_per_scene).view(-1),\n","                batch_split,\n","            )\n","            sdf_gt = torch.chunk(sdf_gt, batch_split)\n","            batch_loss = 0.0\n","            optimizer_all.zero_grad()\n","            for i in range(batch_split):\n","                batch_vecs = lat_vecs(indices[i])\n","                input = torch.cat([batch_vecs, xyz[i]], dim=1)\n","                # NN optimization\n","                pred_sdf = decoder(input)\n","                if enforce_minmax:\n","                    pred_sdf = torch.clamp(pred_sdf, minT, maxT)\n","                chunk_loss = loss_l1(pred_sdf, sdf_gt[i].cuda()) / num_sdf_samples\n","                if do_code_regularization:\n","                    l2_size_loss = torch.sum(torch.norm(batch_vecs, dim=1))\n","                    reg_loss = (\n","                        code_reg_lambda * min(1, epoch / 100) * l2_size_loss\n","                    ) / num_sdf_samples\n","                    chunk_loss = chunk_loss + reg_loss.cuda()\n","                chunk_loss.backward()\n","                batch_loss += chunk_loss.item()\n","            logging.debug(\"loss = {}\".format(batch_loss))\n","            loss_log.append(batch_loss)\n","            if grad_clip is not None:\n","                torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n","            optimizer_all.step()\n","        end = time.time()\n","        seconds_elapsed = end - start\n","        timing_log.append(seconds_elapsed)\n","        if epoch % 50 == 0:\n","            print(f'Epoch {epoch} with {seconds_elapsed} seconds passed')\n","            \n","        lr_log.append([schedule.get_learning_rate(epoch) for schedule in lr_schedules])\n","        lat_mag_log.append(get_mean_latent_vector_magnitude(lat_vecs))\n","        append_parameter_magnitudes(param_mag_log, decoder)\n","        if epoch in checkpoints:\n","            save_checkpoints(epoch)\n","        if epoch % log_frequency == 0:\n","            save_latest(epoch)\n","            save_logs(\n","                experiment_directory,\n","                loss_log,\n","                lr_log,\n","                timing_log,\n","                lat_mag_log,\n","                param_mag_log,\n","                epoch,\n","            )"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training with 1 GPU(s)\n","continuing from \"latest\"\n"]},{"name":"stderr","output_type":"stream","text":["/home/shen/miniconda3/envs/deepSDF/lib/python3.8/site-packages/torch/nn/parallel/comm.py:231: UserWarning: Using -1 to represent CPU tensor is deprecated. Please use a device object or string instead, e.g., \"cpu\".\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 700 with 49.80327320098877 seconds passed\n","Epoch 750 with 46.230857849121094 seconds passed\n","Epoch 800 with 46.03813457489014 seconds passed\n","Epoch 850 with 47.33618474006653 seconds passed\n","Epoch 900 with 45.866782903671265 seconds passed\n","Epoch 950 with 46.17379856109619 seconds passed\n","Epoch 1000 with 45.77881383895874 seconds passed\n","Epoch 1050 with 45.5020706653595 seconds passed\n","Epoch 1100 with 45.793654918670654 seconds passed\n","Epoch 1150 with 51.49678611755371 seconds passed\n","Epoch 1200 with 45.57224988937378 seconds passed\n","Epoch 1250 with 46.03581929206848 seconds passed\n","Epoch 1300 with 46.62713289260864 seconds passed\n","Epoch 1350 with 49.333528995513916 seconds passed\n","Epoch 1400 with 46.673535108566284 seconds passed\n","Epoch 1450 with 45.8773832321167 seconds passed\n","Epoch 1500 with 45.87163805961609 seconds passed\n","Epoch 1550 with 45.702412843704224 seconds passed\n","Epoch 1600 with 45.62686634063721 seconds passed\n","Epoch 1650 with 49.003443002700806 seconds passed\n"]},{"ename":"SystemExit","evalue":"0","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"name":"stderr","output_type":"stream","text":["/home/shen/miniconda3/envs/deepSDF/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["experiment_directory = 'examples/sofas'\n","main_function(experiment_directory, continue_from = 'latest', batch_split=4)"]}],"metadata":{"kernelspec":{"display_name":"deepSDF","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"6b8269542496d3a0fb6a6c002ebf06e81bd408d243997e872c59b60e4b1e04cf"}}},"nbformat":4,"nbformat_minor":2}
